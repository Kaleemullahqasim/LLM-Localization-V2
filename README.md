# Rule-Anchored Localization QA System - MVP

> **An AI-powered system that automatically extracts translation quality rules from style guides and provides explainable, reproducible scoring for translations.**

## ğŸš€ Quick Start

### Prerequisites
1. **LM Studio running locally** with these models:
   - **Chat**: `qwen/qwen3-1.7b` on `http://localhost:1234`
   - **Embeddings**: `text-embedding-qwen3-embedding-8b` on `http://127.0.0.1:1234`

### Start the Backend
```bash
cd backend
python start.py
```

This automatically:
- Creates virtual environment
- Installs dependencies  
- Creates data directories
- Starts FastAPI server on http://localhost:8000

### Test the System
```bash
cd backend
python test_mvp.py
```

## ğŸ—ï¸ System Overview

### What It Does
1. **ğŸ“„ Document Ingestion**: Upload style guides (DOCX/PDF/HTML/MD) â†’ AI extracts rules
2. **ğŸ” Translation Evaluation**: Submit translations â†’ Get scored against rules with explanations
3. **ğŸ‘¥ Human Review**: Reviewers can override AI findings and see score updates in real-time

### Key Features
- **ğŸ¤– LLM-Powered Rule Extraction**: Automatically creates rule knowledge bases from documents
- **âš¡ Deterministic + AI Validation**: Fast mechanical checks + semantic LLM evaluation  
- **ğŸ“Š Explainable Scoring**: Every deduction tied to specific rule with citation
- **ğŸ”„ Reproducible Results**: Same input + version = identical scores
- **ğŸ‘¨â€ğŸ’¼ Human Oversight**: Reviewers maintain control with override capabilities

## ğŸ› ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Style Guide   â”‚â”€â”€â”€â–¶â”‚  Knowledge Base  â”‚â”€â”€â”€â–¶â”‚   Evaluation    â”‚
â”‚  (DOCX/PDF/MD)  â”‚    â”‚  (Rules + Meta)  â”‚    â”‚  (Score Report) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–²                        â–²
                         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ LLM Extract â”‚        â”‚ Deterministic + â”‚
                         â”‚    Rules    â”‚        â”‚  LLM Evaluate   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Pipeline
1. **Ingestion**: Document â†’ LLM â†’ Atomic Rules â†’ Knowledge Base (JSON + CSV)
2. **Evaluation**: Source + Target â†’ Validators + Retrieval + LLM â†’ Findings â†’ Score
3. **Review**: Human Override â†’ Feedback Events â†’ Updated Score

## ğŸ“ Project Structure

```
â”œâ”€â”€ spec/
â”‚   â”œâ”€â”€ PRD.md              # Product Requirements Document
â”‚   â””â”€â”€ tasks.md            # Implementation tasks
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models/         # Data models (Rule, Finding, ScoreReport)
â”‚   â”‚   â”œâ”€â”€ services/       # Core business logic
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI application
â”‚   â”œâ”€â”€ data/               # Generated data (KBs, evaluations)
â”‚   â”œâ”€â”€ start.py            # One-command startup
â”‚   â””â”€â”€ test_mvp.py         # System validation tests
â””â”€â”€ README.md               # This file
```

## ğŸ”§ API Endpoints

### Document Management
- `POST /api/upload-document` - Upload style guide, create KB
- `GET /api/knowledge-bases` - List available knowledge bases

### Evaluation  
- `POST /api/evaluate` - Evaluate translation against KB
- `GET /api/evaluation/{job_id}` - Get detailed results

### Review & Override
- `POST /api/review/override` - Override finding, recalculate score
- `GET /api/rules/search` - Search rules by text

### Utilities
- `GET /api/stats` - System statistics
- `GET /docs` - Interactive API documentation

## ğŸ§ª Acceptance Tests

The MVP passes when:
- âœ… Upload Shopify zh-CN guide â†’ â‰¥30 rules extracted
- âœ… Deterministic validators catch seeded violations:
  - Half-width punctuation in Chinese (! â†’ ï¼)
  - Modified placeholders ({name} â†’ {nome})  
  - Wrong date format (2025-09-01 â†’ 2025å¹´9æœˆ1æ—¥)
- âœ… ScoreReport shows findings with rule citations
- âœ… Reviewer can override severity, see live score update
- âœ… Same KB version â†’ identical reproducible results

## ğŸ¯ MVP Scope

### âœ… Included
- Document ingestion (DOCX/PDF/HTML/MD)
- Rule extraction via LLM with JSON schema
- 5 core deterministic validators  
- Hybrid rule retrieval (embeddings + keywords)
- LLM evaluation with structured output
- Scoring engine with severity multipliers
- Basic review interface via API
- Audit trail with feedback events

### ğŸš« Out of Scope (V2+)
- Advanced UI/frontend
- Multi-language evaluation  
- CAT tool integration
- Analytics dashboards
- Global rubric updates
- Advanced category caps

## ğŸ¤ Contributing

This is an MVP focused on proving the core concept. The system demonstrates:
1. **Automated rule extraction** from style guides
2. **Explainable scoring** with human oversight
3. **Reproducible results** with version control
4. **LLM integration** with local models (LM Studio)

## ğŸš¦ Next Steps

1. **Test with your data**: Upload your style guide and test translations
2. **Frontend development**: Build React UI for easier review workflow  
3. **Performance optimization**: Improve rule retrieval and scoring speed
4. **Enterprise features**: User management, advanced analytics, integrations

---

**Built for**: Translation QA teams who want consistent, explainable, and improvable quality assessment that scales human expertise rather than replacing it.